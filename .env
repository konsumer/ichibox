# default environemnt
# feel free to override any of these

WAKEWORD = "ichibox"

# download these with instructions in README
DEEP_MODEL="models/output_graph.pbmm"
DEEP_ALPHABET="models/alphabet.txt"
DEEP_LM="models/lm.binary"
DEEP_TRIE="models/trie"

# you can mostly just leave these as-is:

# Beam width used in the CTC decoder when building candidate transcriptions
DEEP_BEAM_WIDTH = 500

# The alpha hyperparameter of the CTC decoder. Language Model weight
DEEP_LM_WEIGHT = 1.50

# Valid word insertion weight. This is used to lessen the word insertion penalty
# when the inserted word is part of the vocabulary
DEEP_VALID_WORD_COUNT_WEIGHT = 2.10

# These constants are tied to the shape of the graph used (changing them changes
# the geometry of the first layer), so make sure you use the same constants that
# were used during training

# Number of MFCC features to use
DEEP_N_FEATURES = 26

# Size of the context window used for producing timesteps in the input vector
DEEP_N_CONTEXT = 9